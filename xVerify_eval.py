import json
import os
import argparse
from pathlib import Path
from tqdm import tqdm
import time
from typing import Dict, List, Tuple

from src.xVerify.model import Model
from src.xVerify.eval import Evaluator

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

def load_converted_data(file_path: str) -> List[Dict]:
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def initialize_xverify_model(model_name: str = 'xVerify-0.5B-I', 
                           url: str = 'IAAR-Shanghai/xVerify-0.5B-I',
                           inference_mode: str = 'local',
                           api_key: str = None):
    try:
        model = Model(
            model_name=model_name,
            model_path_or_url=url,
            inference_mode=inference_mode,
            api_key=api_key
        )
        evaluator = Evaluator(model=model, process_num=1)
        print(f"âœ… xVerifyæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name} (å•è¿›ç¨‹æ¨¡å¼)")
        return evaluator
    except Exception as e:
        print(f"âŒ xVerifyæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def batch_evaluate(evaluator: Evaluator, data_path: str, 
                   max_samples: int = None, output_path: str = None) -> Dict:
    try:
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡è¯„ä¼°: {data_path}")
        
        if output_path is None:
            output_path = "data/xverify_results"
        
        os.makedirs(output_path, exist_ok=True)
        
        os.environ["OMP_NUM_THREADS"] = "1"
        os.environ["MKL_NUM_THREADS"] = "1"
        os.environ["NUMEXPR_NUM_THREADS"] = "1"
        
        original_data = load_converted_data(data_path)
        if max_samples:
            original_data = original_data[:max_samples]
        
        temp_data_path = data_path.replace('.json', '_temp.json')
        temp_data = []
        for item in original_data:
            temp_item = {
                "question": item["question"],
                "llm_output": item["llm_output"],
                "correct_answer": item["correct_answer"]
            }
            temp_data.append(temp_item)
        
        with open(temp_data_path, 'w', encoding='utf-8') as f:
            json.dump(temp_data, f, indent=2, ensure_ascii=False)
        
        results = evaluator.evaluate(
            data_path=temp_data_path,
            output_path=output_path,
            data_size=max_samples
        )
        
        if os.path.exists(temp_data_path):
            os.remove(temp_data_path)
        
        if isinstance(results, dict) and "results" in results:
            for i, result in enumerate(results["results"]):
                if i < len(original_data):
                    original_class = original_data[i].get("class", "")
                    if original_class in ["Biochemistry", "Organic"]:
                        result["class"] = "Biochemistry_Organic"
                    else:
                        result["class"] = original_class
                    result["index"] = original_data[i].get("index", i)
        
        print(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ")
        return results
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return {"error": str(e)}

def save_results(results: List[Dict], output_file: str, model_name: str):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    output_data = {
        "model_name": model_name,
        "total_samples": len(results),
        "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "results": results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def analyze_results(results: Dict) -> Dict:
    if "error" in results:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {results['error']}")
        return results
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœåˆ†æ:")
    print(f"è¯„ä¼°ç»“æœ: {results}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨xVerifyæœ¬åœ°æ¨¡å‹è¯„ä¼°è½¬æ¢åçš„æ•°æ®")
    parser.add_argument("--input", "-i", type=str, 
                       default="data/converted_results/USD-guiji_deepseek-r1_converted.json",
                       help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, 
                       default="data/xverify_results",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model-name", type=str, 
                       default="xVerify-0.5B-I",
                       help="xVerifyæ¨¡å‹åç§°")
    parser.add_argument("--url", type=str, 
                       default="IAAR-Shanghai/xVerify-0.5B-I",
                       help="Hugging Faceæ¨¡å‹æ ‡è¯†ç¬¦æˆ–æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--inference-mode", type=str, 
                       default="local",
                       choices=["api", "local"],
                       help="æ¨ç†æ¨¡å¼")
    parser.add_argument("--api-key", type=str, 
                       default=None,
                       help="APIå¯†é’¥")
    parser.add_argument("--max-samples", type=int, 
                       default=1000,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--delay", type=float, 
                       default=0.0,
                       help="è¯·æ±‚é—´éš”å»¶è¿Ÿï¼ˆç§’ï¼Œæœ¬åœ°æ¨¡å¼å»ºè®®è®¾ä¸º0ï¼‰")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    print(f"ğŸ“Š ä½¿ç”¨æ•°æ®æ–‡ä»¶: {args.input}")
    
    print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–xVerifyæ¨¡å‹: {args.model_name}")
    evaluator = initialize_xverify_model(
        model_name=args.model_name,
        url=args.url,
        inference_mode=args.inference_mode,
        api_key=args.api_key
    )
    
    if evaluator is None:
        print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    results = batch_evaluate(
        evaluator=evaluator,
        data_path=args.input,
        max_samples=args.max_samples,
        output_path=args.output
    )
    
    analysis = analyze_results(results)
    
    print(f"ğŸ¯ è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

if __name__ == "__main__":
    main() 